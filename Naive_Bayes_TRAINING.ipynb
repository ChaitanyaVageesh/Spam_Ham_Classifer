{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T08:21:44.178838Z",
     "iopub.status.busy": "2024-10-31T08:21:44.178422Z",
     "iopub.status.idle": "2024-10-31T08:21:44.306227Z",
     "shell.execute_reply": "2024-10-31T08:21:44.304964Z",
     "shell.execute_reply.started": "2024-10-31T08:21:44.178795Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('trial.n.02'), Synset('test.n.02'), Synset('examination.n.02'), Synset('test.n.04'), Synset('test.n.05'), Synset('test.n.06'), Synset('test.v.01'), Synset('screen.v.01'), Synset('quiz.v.01'), Synset('test.v.04'), Synset('test.v.05'), Synset('test.v.06'), Synset('test.v.07')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Test to see if wordnet can find synonyms for a word\n",
    "synonyms = wordnet.synsets(\"test\")\n",
    "print(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T08:19:48.390055Z",
     "iopub.status.busy": "2024-10-31T08:19:48.389645Z",
     "iopub.status.idle": "2024-10-31T08:19:49.653748Z",
     "shell.execute_reply": "2024-10-31T08:19:49.652493Z",
     "shell.execute_reply.started": "2024-10-31T08:19:48.390017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "# from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "# import enchant\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "custom_stop_words = ENGLISH_STOP_WORDS.union({\"please\", \"regards\", \"thank\", \"thanks\",  \"good\", \"morning\", \"afternoon\", \"email\"})\n",
    "domain_specific_spam_words = [\"urgent\", \"bonus\", \"unsubscribe\", \"winner\", \"claim\", \"discount\", \"buy now\", \"free\", \"limited time offer\", \"sold\", \"click\"]\n",
    "domain_specific_ham_words = [\"meeting\", \"schedule\", \"project\", \"client\", \"presentation\", \"follow-up\", \"update\", \"pfa\", \"attached\", \"agreement\", \"termsheet\"]\n",
    "\n",
    "def lemmatize_and_filter_words(text):\n",
    "    doc = nlp(text)\n",
    "    meaningful_words = []\n",
    "    # non_english_words_count = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        # Skip stop words, non-alphabetic tokens, and single-letter tokens\n",
    "        if token.is_stop or not token.is_alpha or len(token) <= 2:\n",
    "            continue\n",
    "        lemma = token.lemma_.lower()\n",
    "        if lemma not in custom_stop_words:\n",
    "            meaningful_words.append(lemma)\n",
    "        \n",
    "    return meaningful_words\n",
    "\n",
    "def extract_words(text):\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "    # Remove punctuation\n",
    "    # text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    # text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    words= lemmatize_and_filter_words(text)\n",
    "\n",
    "    # word_counts = Counter(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def process_emails_text(folder_path):\n",
    "    emails_data = []\n",
    "    punc=[]\n",
    "    for filename in os.listdir(folder_path):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            content = file.read()\n",
    "            punctuation_count = sum([1 for char in content if char in string.punctuation])\n",
    "            \n",
    "            words = extract_words(content)\n",
    "            punc.append(punctuation_count/len(words))\n",
    "            \n",
    "            emails_data.append(words)\n",
    "    return emails_data, punc\n",
    "\n",
    "def process_train_emails(spam_folder, ham_folder, vectorizer):\n",
    "    spam_emails, spam_punc= process_emails_text(spam_folder)   \n",
    "    ham_emails, ham_punc= process_emails_text(ham_folder)\n",
    "    combined_emails = spam_emails + ham_emails\n",
    "    X_vec = vectorizer.fit_transform(combined_emails)\n",
    "    \n",
    "    # Create DataFrame from the TF-IDF matrix\n",
    "    df = pd.DataFrame(X_vec.astype(np.float32).toarray(), columns=vectorizer.get_feature_names_out()).fillna(0)\n",
    "    df['punctuation_percent'] = spam_punc + ham_punc\n",
    "    # Add labels\n",
    "    labels = [1] * len(spam_emails) + [0] * len(ham_emails)\n",
    "    df['Label'] = labels\n",
    "    df= df.astype('float32')\n",
    "    return df\n",
    "\n",
    "def process_test_emails(spam_folder, ham_folder, vectorizer):\n",
    "    spam_emails, spam_punc= process_emails_text(spam_folder)   \n",
    "    ham_emails, ham_punc= process_emails_text(ham_folder)\n",
    "    combined_emails = spam_emails + ham_emails\n",
    "    X_vec = vectorizer.transform(combined_emails)\n",
    "    \n",
    "    # Create DataFrame from the TF-IDF matrix\n",
    "    df = pd.DataFrame(X_vec.astype(np.float32).toarray(), columns=vectorizer.get_feature_names_out()).fillna(0)\n",
    "    df['punctuation_percent'] = spam_punc + ham_punc\n",
    "    # Add labels\n",
    "    labels = [1] * len(spam_emails) + [0] * len(ham_emails)\n",
    "    df['Label'] = labels\n",
    "    df= df.astype('float32')\n",
    "    return df\n",
    "    \n",
    "vectorizer= TfidfVectorizer(min_df=0.0009,max_df=0.9, stop_words=list(custom_stop_words))\n",
    "spam_folder = r'archive/enron1/spam'\n",
    "ham_folder = r'archive/enron1/ham'\n",
    "combined_df = process_train_emails(spam_folder, ham_folder, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer saved to tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('3.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "print(\"Vectorizer saved to tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T07:44:23.240102Z",
     "iopub.status.busy": "2024-10-31T07:44:23.239394Z",
     "iopub.status.idle": "2024-10-31T07:44:23.246019Z",
     "shell.execute_reply": "2024-10-31T07:44:23.245111Z",
     "shell.execute_reply.started": "2024-10-31T07:44:23.240048Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 8334)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = combined_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T07:42:30.746705Z",
     "iopub.status.busy": "2024-10-31T07:42:30.746374Z",
     "iopub.status.idle": "2024-10-31T07:42:30.769894Z",
     "shell.execute_reply": "2024-10-31T07:42:30.768825Z",
     "shell.execute_reply.started": "2024-10-31T07:42:30.746673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "column_names = combined_df.columns.tolist()\n",
    "\n",
    "spam_dict = {col: 0.1 for col in column_names}\n",
    "ham_dict = {col: 0.1 for col in column_names}\n",
    "column_names= [col for col in column_names if col!=\"Label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T16:10:46.776948Z",
     "iopub.status.busy": "2024-10-28T16:10:46.775961Z",
     "iopub.status.idle": "2024-10-28T16:19:01.785342Z",
     "shell.execute_reply": "2024-10-28T16:19:01.782999Z",
     "shell.execute_reply.started": "2024-10-28T16:10:46.776875Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# count={col: 0 for col in column_names}\n",
    "for i, row in combined_df.iterrows():\n",
    "    if row[\"Label\"]==1:\n",
    "    \n",
    "        for word in column_names:\n",
    "            if row[word] >0:\n",
    "                spam_dict[word] = spam_dict[word]+ row[word]\n",
    "                if word in domain_specific_spam_words:\n",
    "                    spam_dict[word] += row[word]\n",
    "#                 \n",
    "    else:\n",
    "        for word in column_names:\n",
    "            if row[word] >0:\n",
    "                ham_dict[word] = ham_dict[word]+ row[word]\n",
    "                if word in domain_specific_ham_words:\n",
    "                    ham_dict[word] += row[word]\n",
    "                # print(ham_dict[word], word)\n",
    "        \n",
    "# for i, row in combined_df.iterrows():\n",
    "#     for word in column_names:\n",
    "#         if row[word]>0:\n",
    "#             word_count = row[word]\n",
    "#             # count[word] += word_count\n",
    "#             if row[\"Label\"] == 1:  # Spam\n",
    "#                 spam_dict[word] += word_count\n",
    "#             else:  # Ham\n",
    "#                 ham_dict[word] += word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-28T16:06:01.528983Z",
     "iopub.status.idle": "2024-10-28T16:06:01.530495Z",
     "shell.execute_reply": "2024-10-28T16:06:01.530150Z",
     "shell.execute_reply.started": "2024-10-28T16:06:01.530110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# P(spam/ word) = P(word/ spam)*P(spam) / P(word)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T16:20:40.657050Z",
     "iopub.status.busy": "2024-10-28T16:20:40.656345Z",
     "iopub.status.idle": "2024-10-28T16:20:40.816414Z",
     "shell.execute_reply": "2024-10-28T16:20:40.814689Z",
     "shell.execute_reply.started": "2024-10-28T16:20:40.656993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "P_spam= sum(combined_df[\"Label\"]==1)/len(combined_df)\n",
    "vocabulary_size = len(column_names)\n",
    "total_words= len(column_names)\n",
    "total_spam_count = sum(spam_dict.values())\n",
    "total_ham_count = sum(ham_dict.values())\n",
    "\n",
    "P_word_spam = {word: ((spam_dict[word])/(total_spam_count))  for word in column_names}\n",
    "P_word_ham = {word: ((ham_dict[word])/(total_ham_count)) for word in column_names}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to ham_words_probab\n",
      "Dictionary saved to spam_words_probab\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('ham_words_probab3.json', \"w\") as file:\n",
    "    json.dump(P_word_ham, file)\n",
    "\n",
    "print(\"Dictionary saved to\", 'ham_words_probab')\n",
    "\n",
    "with open('spam_words_probab3.json', \"w\") as file:\n",
    "    json.dump(P_word_spam, file)\n",
    "\n",
    "print(\"Dictionary saved to\", 'spam_words_probab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spam_path= r'archive/enron2/spam'\n",
    "test_ham_path= r'archive/enron2/ham'\n",
    "\n",
    "combined_test_df = process_test_emails(test_spam_path, test_ham_path, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n",
    "def is_spam(df,P_word_spam, P_word_ham, threshold):\n",
    "    # column_names_test = df.columns.tolist()\n",
    "    training_words = set(P_word_spam.keys()).union(set(P_word_ham.keys()))\n",
    "    spam_scores, ham_scores=[],[]\n",
    "    for i, row in df.iterrows():\n",
    "    \n",
    "        spam_score, ham_score=np.log(P_spam),np.log(1- P_spam)\n",
    "        for word in training_words:\n",
    "            word_count= row.get(word,0)\n",
    "            if word_count>0:\n",
    "                if word in P_word_spam:\n",
    "                    spam_score += word_count * np.log(P_word_spam[word])\n",
    "                if word in P_word_ham:\n",
    "                    ham_score += word_count * np.log(P_word_ham[word])\n",
    "            # else:\n",
    "            #     if word in P_word_spam:\n",
    "            #         spam_score += np.log(1- P_word_spam[word])\n",
    "            #     if wor d in P_word_ham:\n",
    "            #         ham_score += np.log(1 - P_word_ham[word])\n",
    "    \n",
    "        spam_scores.append(spam_score)\n",
    "        ham_scores.append(ham_score)\n",
    "    classifications = [1 if spam_score > ham_score + threshold else 0 for spam_score, ham_score in zip(spam_scores, ham_scores)]\n",
    "    accuracy= accuracy_score(combined_test_df[\"Label\"].tolist(), classifications)\n",
    "    f1= f1_score(combined_test_df[\"Label\"].tolist(), classifications, average= 'micro')\n",
    "    bal= balanced_accuracy_score(combined_test_df[\"Label\"].tolist(), classifications)\n",
    "    print('Accuracy is',accuracy,',f1 micro score is',f1,' and Balanced Accuracy is', bal)\n",
    "    return spam_scores, ham_score, classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9682431278811678 ,f1 micro score is 0.9682431278811678  and Balanced Accuracy is 0.9635241481679495\n"
     ]
    }
   ],
   "source": [
    "spam_scores_1, ham_scores_1, classifications_1 =is_spam(combined_test_df,P_word_spam, P_word_ham, 0 )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 423767,
     "sourceId": 808042,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
