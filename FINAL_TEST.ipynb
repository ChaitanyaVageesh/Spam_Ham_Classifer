{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer loaded successfully.\n",
      "Dictionaries loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "# from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "# import enchant\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "# from nltk.corpus import wordnet\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Step 1: Load the probability dictionaries\n",
    "with open('ham_words_probab3.json', \"r\") as file:\n",
    "    P_word_ham = json.load(file)\n",
    "\n",
    "with open('spam_words_probab3.json', \"r\") as file:\n",
    "    P_word_spam = json.load(file)\n",
    "\n",
    "with open('tfidf3.pkl', 'rb') as file:\n",
    "    vectorizer = pickle.load(file)\n",
    "print(\"Vectorizer loaded successfully.\")\n",
    "\n",
    "print(\"Dictionaries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize_and_filter_words(text):\n",
    "    doc = nlp(text)\n",
    "    meaningful_words = []\n",
    "    # non_english_words_count = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        # Skip stop words, non-alphabetic tokens, and single-letter tokens\n",
    "        if token.is_stop or not token.is_alpha or len(token) <= 2:\n",
    "            continue\n",
    "        lemma = token.lemma_.lower()\n",
    "        meaningful_words.append(lemma)\n",
    "        \n",
    "    return meaningful_words\n",
    "\n",
    "def extract_words(text):\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "    # Remove punctuation\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    words= lemmatize_and_filter_words(text)\n",
    "\n",
    "    # word_counts = Counter(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# def process_emails_text(folder_path):\n",
    "#     emails_data = []\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         with open(os.path.join(folder_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
    "#             content = file.read()\n",
    "#             words = extract_words(content)\n",
    "            \n",
    "#             emails_data.append(words)\n",
    "#     return emails_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_emails_text(folder_path):\n",
    "    emails_data = []\n",
    "    punc=[]\n",
    "    for filename in os.listdir(folder_path):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            content = file.read()\n",
    "            punctuation_count = sum([1 for char in content if char in string.punctuation])\n",
    "            \n",
    "            words = extract_words(content)\n",
    "            punc.append(punctuation_count/len(words))\n",
    "            \n",
    "            emails_data.append(words)\n",
    "    return emails_data, punc\n",
    "\n",
    "def process_test_emails(spam_folder, vectorizer):\n",
    "    emails, punc= process_emails_text(spam_folder)   \n",
    "    \n",
    "    X_vec = vectorizer.transform(emails)\n",
    "    \n",
    "    # Create DataFrame from the TF-IDF matrix\n",
    "    df = pd.DataFrame(X_vec.toarray(), columns=vectorizer.get_feature_names_out()).fillna(0)\n",
    "    df['punctuation_percent'] = punc\n",
    "    df= df.astype('float32')\n",
    "    return df\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_spam(df,P_word_spam, P_word_ham, threshold):\n",
    "   \n",
    "    training_words = set(P_word_spam.keys()).union(set(P_word_ham.keys()))\n",
    "    spam_scores, ham_scores=[],[]\n",
    "    for i, row in df.iterrows():\n",
    "        P_spam= 0.39\n",
    "        spam_score, ham_score=np.log(P_spam),np.log(1- P_spam)\n",
    "        for word in training_words:\n",
    "            # if word in P_word_spam:\n",
    "            #     spam_score += np.log(P_word_spam[word]) * row[word]\n",
    "            # else:\n",
    "            #     print(f\"Current count value: {count}, type: {type(count)}\")\n",
    "            #     spam_score += np.log(1e-6) *row[count]\n",
    "            # if word in P_word_ham:\n",
    "            #     ham_score+= np.log(P_word_ham[word]) * row[word]\n",
    "            # else:\n",
    "            #     ham_score += np.log(1e-6) *row[count[word]]\n",
    "            word_count= row.get(word,0)\n",
    "            if word_count>0:\n",
    "                if word in P_word_spam:\n",
    "                    spam_score += word_count * np.log(P_word_spam[word])\n",
    "                if word in P_word_ham:\n",
    "                    ham_score += word_count * np.log(P_word_ham[word])\n",
    "            # else:\n",
    "            #     if word in P_word_spam:\n",
    "            #         spam_score += np.log(1- P_word_spam[word])\n",
    "            #     if word in P_word_ham:\n",
    "            #         ham_score += np.log(1 - P_word_ham[word])\n",
    "    \n",
    "        spam_scores.append(spam_score)\n",
    "        ham_scores.append(ham_score)\n",
    "    classifications = [1 if spam_score > ham_score + threshold else 0 for spam_score, ham_score in zip(spam_scores, ham_scores)]\n",
    "\n",
    "    return spam_scores, ham_score, classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spam_path= 'test'\n",
    "emails_data = []\n",
    "err = 0\n",
    "\n",
    "test_df= process_test_emails(test_spam_path, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_, classifications= is_spam(test_df,P_word_spam, P_word_ham, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame(classifications)\n",
    "     \n",
    "# saving the dataframe\n",
    "df_out.to_csv('predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
