{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6399975,"sourceType":"datasetVersion","datasetId":3690036}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nimport spacy\nfrom gensim.models import Word2Vec\nnlp = spacy.load('en_core_web_sm')\n\ndf= pd.read_csv(r'/kaggle/input/spam-email-dataset/emails.csv', )\ny= df['spam'].values\nX= df['text']\n\n\ndef preprocess(text):\n    text = re.sub(r'[^\\w\\s]', '', text.lower())\n    doc= nlp(text)\n    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n\n# X= X.apply(preprocess).tolist()","metadata":{"execution":{"iopub.status.busy":"2024-11-03T09:40:16.643425Z","iopub.execute_input":"2024-11-03T09:40:16.643747Z","iopub.status.idle":"2024-11-03T09:40:42.025121Z","shell.execute_reply.started":"2024-11-03T09:40:16.643706Z","shell.execute_reply":"2024-11-03T09:40:42.023906Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"X_processed = X.apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T09:40:42.027654Z","iopub.execute_input":"2024-11-03T09:40:42.028631Z","iopub.status.idle":"2024-11-03T09:45:52.348511Z","shell.execute_reply.started":"2024-11-03T09:40:42.028574Z","shell.execute_reply":"2024-11-03T09:45:52.347578Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"len(X_processed.tolist()[2])","metadata":{"execution":{"iopub.status.busy":"2024-11-03T09:45:52.349990Z","iopub.execute_input":"2024-11-03T09:45:52.350332Z","iopub.status.idle":"2024-11-03T09:45:52.358247Z","shell.execute_reply.started":"2024-11-03T09:45:52.350279Z","shell.execute_reply":"2024-11-03T09:45:52.357276Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"55"},"metadata":{}}]},{"cell_type":"code","source":"X_processed.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-03T09:45:52.360505Z","iopub.execute_input":"2024-11-03T09:45:52.360855Z","iopub.status.idle":"2024-11-03T09:45:52.372841Z","shell.execute_reply.started":"2024-11-03T09:45:52.360819Z","shell.execute_reply":"2024-11-03T09:45:52.371737Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0    [subject, naturally, irresistible, corporate, ...\n1    [subject, stock, trading, gunslinger,  , fanny...\n2    [subject, unbelievable, new, home, easy,  , m,...\n3    [subject, 4, color, printing, special,  , requ...\n4    [subject, money,  , software, cd,   , software...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"word2vec_model = Word2Vec(sentences=X_processed.tolist(), vector_size=100, window=5, min_count=1, workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T09:45:52.374112Z","iopub.execute_input":"2024-11-03T09:45:52.374469Z","iopub.status.idle":"2024-11-03T09:45:57.818440Z","shell.execute_reply.started":"2024-11-03T09:45:52.374432Z","shell.execute_reply":"2024-11-03T09:45:57.817427Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_email_embedding(words):\n    if not words:  # If the list is empty\n        return np.zeros(word2vec_model.vector_size)  # Return a zero vector\n    # Get the embeddings for each word in the email and calculate the mean\n    return np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv], axis=0)\n\n# Apply the embedding function to get embeddings for each email\nX_embeddings = np.array(X_processed.apply(get_email_embedding).tolist())\n","metadata":{"execution":{"iopub.status.busy":"2024-11-03T09:45:57.819609Z","iopub.execute_input":"2024-11-03T09:45:57.819906Z","iopub.status.idle":"2024-11-03T09:46:02.291432Z","shell.execute_reply.started":"2024-11-03T09:45:57.819873Z","shell.execute_reply":"2024-11-03T09:46:02.290608Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(X_embeddings.shape)\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T09:46:02.292566Z","iopub.execute_input":"2024-11-03T09:46:02.292895Z","iopub.status.idle":"2024-11-03T09:46:02.302455Z","shell.execute_reply.started":"2024-11-03T09:46:02.292860Z","shell.execute_reply":"2024-11-03T09:46:02.301502Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(5728, 100)\n","output_type":"stream"}]},{"cell_type":"code","source":"class DecisionStump:\n    def __init__(self):\n        # The feature index to split on\n        self.feature_idx = None\n        # The threshold value for splitting\n        self.threshold = None\n        # Whether to predict +1 or -1 for values above threshold\n        self.polarity = None\n        \n    def fit(self, X, y, sample_weight=None):\n        if sample_weight is None:\n            sample_weight = np.ones(len(y))\n            \n        n_samples, n_features = X.shape\n        min_error = float('inf')\n        \n        # For each feature\n        for feature in range(n_features):\n            # Get all values for this feature\n            feature_values = X[:, feature]\n            thresholds = np.unique(feature_values)\n            \n            # Try each value as a threshold\n            for threshold in thresholds:\n                # Predict 1 for values above threshold, -1 for below\n                pred1 = np.ones(n_samples)\n                pred1[feature_values < threshold] = -1\n                error1 = np.sum(sample_weight * (pred1 != y)) / np.sum(sample_weight)\n                \n                # Predict -1 for values above threshold, 1 for below\n                pred2 = np.ones(n_samples)\n                pred2[feature_values >= threshold] = -1\n                error2 = np.sum(sample_weight * (pred2 != y)) / np.sum(sample_weight)\n                \n                # Keep track of the best split\n                if error1 < min_error:\n                    min_error = error1\n                    self.feature_idx = feature\n                    self.threshold = threshold\n                    self.polarity = 1\n                    \n                if error2 < min_error:\n                    min_error = error2\n                    self.feature_idx = feature\n                    self.threshold = threshold\n                    self.polarity = -1\n        \n        return self\n    \n    def predict(self, X):\n        predictions = np.ones(X.shape[0])\n        if self.polarity == 1:\n            predictions[X[:, self.feature_idx] < self.threshold] = -1\n        else:\n            predictions[X[:, self.feature_idx] >= self.threshold] = -1\n        return predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-11-03T09:46:02.304059Z","iopub.execute_input":"2024-11-03T09:46:02.304923Z","iopub.status.idle":"2024-11-03T09:46:02.311504Z","shell.execute_reply.started":"2024-11-03T09:46:02.304852Z","shell.execute_reply":"2024-11-03T09:46:02.310644Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Adaboost():\n    \"\"\"Boosting method that uses a number of weak classifiers in \n    ensemble to make a strong classifier. This implementation uses decision\n    stumps, which is a one level Decision Tree. \n\n    Parameters:\n    -----------\n    n_clf: int\n        The number of weak classifiers that will be used. \n    \"\"\"\n    def __init__(self,  weak_learner,EXPLICIT_SAMPLING, n_clf=50, eta=0.5):\n        self.n_clf = n_clf\n        self.weak_learner= weak_learner\n#         self.params= params if params is not None else {}\n        self.EXPLICIT_SAMPLING= EXPLICIT_SAMPLING\n        self.eta = eta\n        self.clfs = []  # To store weak classifiers\n        self.alphas = [] \n\n    def fit(self, X, y):\n        m,n= X.shape\n        ids= np.arange(m)\n        w= np.ones(m)/m\n        if np.isfinite(X).all() == False:\n            raise ValueError(\"Input contains NaN or Infinite values.\")\n        if m <= 1 or n >= m:\n            raise ValueError(\"More samples than features required.\")\n        for _ in range(self.n_clf):\n\n            clf= self.weak_learner()\n            if self.EXPLICIT_SAMPLING:\n                sampled_ids = np.random.choice(ids, size=m, p=w)\n                X_iter = X[sampled_ids]\n                y_iter = y[sampled_ids]\n                clf.fit(X_iter, y_iter)\n                pred= clf.predict(X_iter)\n             \n            \n            else:\n                clf.fit(X, y, sample_weight= w)\n                pred= clf.predict(X)\n            \n       \n            # err= np.sum(w[y != pred])\n            # err= w[pred != y_train].sum() / w.sum()\n            err= np.sum(w * (pred != y)) / np.sum(w)\n            print(err)\n            alpha= self.eta* np.log((1-err)/(err+ 1e-10) )\n            # if err!=0 else 0\n            # alpha = max(0, min(1, alpha))  # Ensures alpha stays between 0 and 1\n\n        \n            w= w*np.exp(-alpha*y*pred)\n            # w= w/np.sum(w)\n            # w[pred!=y_train]*= np.exp(alpha)\n            # w[pred==y_train]*= np.exp(-alpha)\n            w/= np.sum(w)\n            self.clfs.append(clf)\n            self.alphas.append(alpha)\n   \n            # plot_decision_boundary(clf, X, y)\n            \n    def predict(self, X):\n        final_pred= np.zeros(X.shape[0])\n        for alpha, learner in zip(self.alphas, self.clfs):\n            final_pred += alpha * learner.predict(X)\n        # print('inpred',np.sign(final_pred))\n        # print('truth',y_train)\n        return np.sign(final_pred)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T09:46:02.337293Z","iopub.execute_input":"2024-11-03T09:46:02.337619Z","iopub.status.idle":"2024-11-03T09:46:02.352175Z","shell.execute_reply.started":"2024-11-03T09:46:02.337585Z","shell.execute_reply":"2024-11-03T09:46:02.351183Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\n\n# Create AdaBoost classifier with DecisionStump\nboost = Adaboost(\n    weak_learner=DecisionStump,\n\n    EXPLICIT_SAMPLING=False,\n    n_clf=50,\n    eta=0.5\n)\n\n# Train the model\nboost.fit(X_train, y_train)\n\n# Make predictions\ny_pred = boost.predict(X_test)\n\n# Calculate accuracy\naccuracy = np.mean(y_pred == y_test)\nprint(f\"Training accuracy: {accuracy:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}